{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = pd.read_csv(\"data/resume.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out the unique category found in the resume csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = df_resume.reindex(np.random.permutation(df_resume.index))\n",
    "df_resume = df_resume.copy().iloc[:1000, ]\n",
    "df_resume.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load skill data\n",
    "\n",
    "If we define patterns for all the skill, we gonna be too tired.\n",
    "\n",
    "So spacy knows that, so it allows you to give you a list of words, then it will automatically create pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "skill_path = 'data/skills.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_path)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Chaky loves ajax.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's try to extract skills from this resume.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean our data by removing stop words and punctuation symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean our data\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    stopwords    = list(STOP_WORDS)\n",
    "    doc          = nlp(sentence)\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'SPACE':\n",
    "                clean_tokens.append(token.lemma_.lower().strip())\n",
    "                \n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_resume = df_resume.Resume_str.iloc[5]\n",
    "random_resume[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(random_resume[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_resume.iterrows():\n",
    "    clean_text = preprocessing(row.Resume_str)\n",
    "    df_resume.at[i, 'Clean_resume'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let's really extract skills!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    skills = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'SKILL':\n",
    "            skills.append(ent.text)\n",
    "            \n",
    "    return skills\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(resume):\n",
    "    \n",
    "    doc = nlp(resume)\n",
    "\n",
    "    entities={}\n",
    "    \n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in entities:\n",
    "            entities[entity.label_].append(entity.text)\n",
    "        else:\n",
    "            entities[entity.label_] = [entity.text]\n",
    "    for ent_type in entities.keys():\n",
    "        entities[ent_type]=', '.join(unique_entities(entities[ent_type]))\n",
    "    return entities\n",
    "\n",
    "def unique_entities(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume['Skills'] = df_resume.Clean_resume.apply(get_skills)\n",
    "df_resume['Skills'] = df_resume.Skills.apply(unique_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.Skills.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Which skills is most important in information management?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'INFORMATION-TECHNOLOGY'\n",
    "cond = df_resume.Category == category\n",
    "\n",
    "df_resume_it = df_resume[cond]\n",
    "df_resume_it.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = np.concatenate(df_resume_it.Skills.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "counting = Counter(all_skills)\n",
    "counting = OrderedDict(counting.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.bar(counting.keys(), counting.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Let's load the PDF - add some realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "reader = PdfReader(\"data/chaklam_resume.pdf\")\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text()\n",
    "text = preprocessing(text)\n",
    "doc = nlp(text)\n",
    "colors = {\"SKILL\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
    "options = {\"colors\": colors}\n",
    "\n",
    "displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
